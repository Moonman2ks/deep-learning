{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exxon Mobil Corporation\n",
    "\n",
    "### Stock prediction with deep learning\n",
    "\n",
    "<br/> Gytis Kazlauskis KT-8/2\n",
    "<br/> email: gytis.ka8869@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jumpvine.net/wp-content/uploads/Steps-to-Creating-Company-Culture-e1508926692787.jpg\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/><b>1. Data loading and pre-processing</b>\n",
    "<br/>1.1 Importing the main libraries\n",
    "<br/>1.2 Importing the dataset\n",
    "<br/>1.3 Feature scaling\n",
    "<br/>1.4 Splitting the training set to dependent and independent variables\n",
    "<br/>1.5 Reshaping the matrix\n",
    "<br/><b>2. Building the RNN</b>\n",
    "<br/>2.1 RNN initialization\n",
    "<br/>2.2 Adding the first layer\n",
    "<br/>2.3 Adding 5 more layers\n",
    "<br/>2.4 Adding the output layer & compiling\n",
    "<br/><b>3. Train and deploy the RNN</b>\n",
    "<br/><b>4. Improving the RNN</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/jpeg;base64,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\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 10-year history of the Exxon Mobil prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the main libraries\n",
    "\n",
    "- [Numpy](http://www.numpy.org): library for the Python programming language\n",
    "- [matplotlib](https://matplotlib.org): a plotting library for the Python programming language and its numerical mathematics extension NumPy.\n",
    "- [pandas](https://pandas.pydata.org): software library written for the Python programming language for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the 3 main libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the Exxon Mobil from July 2009 to July 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('dataset_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2009-07-30</td>\n",
       "      <td>70.300003</td>\n",
       "      <td>71.519997</td>\n",
       "      <td>69.910004</td>\n",
       "      <td>70.720001</td>\n",
       "      <td>51.192154</td>\n",
       "      <td>37842300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2009-07-31</td>\n",
       "      <td>70.360001</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>69.430000</td>\n",
       "      <td>70.389999</td>\n",
       "      <td>50.953278</td>\n",
       "      <td>28070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2009-08-03</td>\n",
       "      <td>70.959999</td>\n",
       "      <td>71.400002</td>\n",
       "      <td>70.300003</td>\n",
       "      <td>70.650002</td>\n",
       "      <td>51.141487</td>\n",
       "      <td>24756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2009-08-04</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>70.639999</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.599998</td>\n",
       "      <td>51.105297</td>\n",
       "      <td>18612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2009-08-05</td>\n",
       "      <td>70.480003</td>\n",
       "      <td>70.489998</td>\n",
       "      <td>69.610001</td>\n",
       "      <td>70.029999</td>\n",
       "      <td>50.692692</td>\n",
       "      <td>20348800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2009-07-30  70.300003  71.519997  69.910004  70.720001  51.192154  37842300\n",
       "1  2009-07-31  70.360001  70.500000  69.430000  70.389999  50.953278  28070600\n",
       "2  2009-08-03  70.959999  71.400002  70.300003  70.650002  51.141487  24756800\n",
       "3  2009-08-04  70.250000  70.639999  70.000000  70.599998  51.105297  18612000\n",
       "4  2009-08-05  70.480003  70.489998  69.610001  70.029999  50.692692  20348800"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70.300003],\n",
       "       [70.360001],\n",
       "       [70.959999],\n",
       "       ...,\n",
       "       [74.870003],\n",
       "       [74.879997],\n",
       "       [75.059998]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "The next step is to rescale our data to the range from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28274133],\n",
       "       [0.28400259],\n",
       "       [0.29661554],\n",
       "       ...,\n",
       "       [0.37881028],\n",
       "       [0.37902037],\n",
       "       [0.38280429]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables\n",
    "Importing data from different sources is fundamental to data science and machine learning. The abundance of good quality data not only eliminates a lot of pre-processing steps but also determines how likely your model is going to succeed in predicting plausible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "\n",
    "# the 60 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2517, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 120 because that is the first instance allowing us to \n",
    "# go back 120 days\n",
    "for i in range(120, 2517): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 120 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-120:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29514402, 0.25772552, 0.28021867, ..., 0.26907723, 0.24994745,\n",
       "        0.24616355],\n",
       "       [0.25772552, 0.28021867, 0.27349176, ..., 0.24994745, 0.24616355,\n",
       "        0.23691413],\n",
       "       [0.28021867, 0.27349176, 0.27538362, ..., 0.24616355, 0.23691413,\n",
       "        0.20327936],\n",
       "       ...,\n",
       "       [0.51986555, 0.50241753, 0.50367879, ..., 0.38469632, 0.38974145,\n",
       "        0.3979399 ],\n",
       "       [0.50241753, 0.50367879, 0.51250799, ..., 0.38974145, 0.3979399 ,\n",
       "        0.37881028],\n",
       "       [0.50367879, 0.51250799, 0.48665129, ..., 0.3979399 , 0.37881028,\n",
       "        0.37902037]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "A new matrix dimension is needed to accommodate the indicator (predictor). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/03/22/17/34/cubic-684961_1280.jpg\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the first layer\n",
    "\n",
    "<br/>We first add an object of the LSTM /class! \n",
    "<br/><b>Specifications:</b>\n",
    "<br/>100 neurons\n",
    "<br/>return requences = true\n",
    "<br/>input shape : 3D\n",
    "<br/>dropout rate = 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "Adding 5 more layers, with 100 neurons and the dropout rate of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 100))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2397/2397 [==============================] - 149s 62ms/step - loss: 0.0647\n",
      "Epoch 2/100\n",
      "2397/2397 [==============================] - 133s 56ms/step - loss: 0.0229\n",
      "Epoch 3/100\n",
      "2397/2397 [==============================] - 138s 57ms/step - loss: 0.0132\n",
      "Epoch 4/100\n",
      "2397/2397 [==============================] - 138s 58ms/step - loss: 0.0097\n",
      "Epoch 5/100\n",
      "2397/2397 [==============================] - 147s 61ms/step - loss: 0.0080\n",
      "Epoch 6/100\n",
      "2397/2397 [==============================] - 139s 58ms/step - loss: 0.0071\n",
      "Epoch 7/100\n",
      "2397/2397 [==============================] - 138s 58ms/step - loss: 0.0061\n",
      "Epoch 8/100\n",
      "2397/2397 [==============================] - 145s 60ms/step - loss: 0.0053\n",
      "Epoch 9/100\n",
      "2397/2397 [==============================] - 141s 59ms/step - loss: 0.0049\n",
      "Epoch 10/100\n",
      "2397/2397 [==============================] - 141s 59ms/step - loss: 0.0047\n",
      "Epoch 11/100\n",
      "2397/2397 [==============================] - 135s 56ms/step - loss: 0.0042\n",
      "Epoch 12/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0040\n",
      "Epoch 13/100\n",
      "2397/2397 [==============================] - 145s 60ms/step - loss: 0.0037\n",
      "Epoch 14/100\n",
      "2397/2397 [==============================] - 141s 59ms/step - loss: 0.0035\n",
      "Epoch 15/100\n",
      "2397/2397 [==============================] - 136s 57ms/step - loss: 0.0032\n",
      "Epoch 16/100\n",
      "2397/2397 [==============================] - 132s 55ms/step - loss: 0.0032\n",
      "Epoch 17/100\n",
      "2397/2397 [==============================] - 134s 56ms/step - loss: 0.0032\n",
      "Epoch 18/100\n",
      "2397/2397 [==============================] - 137s 57ms/step - loss: 0.0029\n",
      "Epoch 19/100\n",
      "2397/2397 [==============================] - 137s 57ms/step - loss: 0.0027\n",
      "Epoch 20/100\n",
      "2397/2397 [==============================] - 139s 58ms/step - loss: 0.0026\n",
      "Epoch 21/100\n",
      "2397/2397 [==============================] - 137s 57ms/step - loss: 0.0026\n",
      "Epoch 22/100\n",
      "2397/2397 [==============================] - 140s 58ms/step - loss: 0.0026\n",
      "Epoch 23/100\n",
      "2397/2397 [==============================] - 134s 56ms/step - loss: 0.0025\n",
      "Epoch 24/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0025\n",
      "Epoch 25/100\n",
      "2397/2397 [==============================] - 132s 55ms/step - loss: 0.0025\n",
      "Epoch 26/100\n",
      "2397/2397 [==============================] - 131s 54ms/step - loss: 0.0025\n",
      "Epoch 27/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0023\n",
      "Epoch 28/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0022\n",
      "Epoch 29/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0023\n",
      "Epoch 30/100\n",
      "2397/2397 [==============================] - 132s 55ms/step - loss: 0.0023\n",
      "Epoch 31/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0022\n",
      "Epoch 32/100\n",
      "2397/2397 [==============================] - 131s 54ms/step - loss: 0.0021\n",
      "Epoch 33/100\n",
      "2397/2397 [==============================] - 131s 54ms/step - loss: 0.0020\n",
      "Epoch 34/100\n",
      "2397/2397 [==============================] - 130s 54ms/step - loss: 0.0019\n",
      "Epoch 35/100\n",
      "2397/2397 [==============================] - 129s 54ms/step - loss: 0.0019\n",
      "Epoch 36/100\n",
      "2397/2397 [==============================] - 129s 54ms/step - loss: 0.0020\n",
      "Epoch 37/100\n",
      "2397/2397 [==============================] - 129s 54ms/step - loss: 0.0019\n",
      "Epoch 38/100\n",
      "2397/2397 [==============================] - 130s 54ms/step - loss: 0.0019\n",
      "Epoch 39/100\n",
      "2397/2397 [==============================] - 136s 57ms/step - loss: 0.0019\n",
      "Epoch 40/100\n",
      "2397/2397 [==============================] - 132s 55ms/step - loss: 0.0018\n",
      "Epoch 41/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0018\n",
      "Epoch 42/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0018\n",
      "Epoch 43/100\n",
      "2397/2397 [==============================] - 134s 56ms/step - loss: 0.0017\n",
      "Epoch 44/100\n",
      "2397/2397 [==============================] - 149s 62ms/step - loss: 0.0017\n",
      "Epoch 45/100\n",
      "2397/2397 [==============================] - 169s 71ms/step - loss: 0.0018\n",
      "Epoch 46/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0017\n",
      "Epoch 47/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0017\n",
      "Epoch 48/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0017\n",
      "Epoch 49/100\n",
      "2397/2397 [==============================] - 130s 54ms/step - loss: 0.0016\n",
      "Epoch 50/100\n",
      "2397/2397 [==============================] - 131s 55ms/step - loss: 0.0016\n",
      "Epoch 51/100\n",
      "2397/2397 [==============================] - 132s 55ms/step - loss: 0.0017\n",
      "Epoch 52/100\n",
      "2397/2397 [==============================] - 134s 56ms/step - loss: 0.0016\n",
      "Epoch 53/100\n",
      "2397/2397 [==============================] - 159s 66ms/step - loss: 0.0015\n",
      "Epoch 54/100\n",
      "2397/2397 [==============================] - 157s 66ms/step - loss: 0.0015\n",
      "Epoch 55/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0015\n",
      "Epoch 56/100\n",
      "2397/2397 [==============================] - 155s 64ms/step - loss: 0.0016\n",
      "Epoch 57/100\n",
      "2397/2397 [==============================] - 153s 64ms/step - loss: 0.0015\n",
      "Epoch 58/100\n",
      "2397/2397 [==============================] - 153s 64ms/step - loss: 0.0016\n",
      "Epoch 59/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0014\n",
      "Epoch 60/100\n",
      "2397/2397 [==============================] - 161s 67ms/step - loss: 0.0014\n",
      "Epoch 61/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0015\n",
      "Epoch 62/100\n",
      "2397/2397 [==============================] - 151s 63ms/step - loss: 0.0015\n",
      "Epoch 63/100\n",
      "2397/2397 [==============================] - 150s 63ms/step - loss: 0.0014\n",
      "Epoch 64/100\n",
      "2397/2397 [==============================] - 150s 63ms/step - loss: 0.0015\n",
      "Epoch 65/100\n",
      "2397/2397 [==============================] - 155s 65ms/step - loss: 0.0015\n",
      "Epoch 66/100\n",
      "2397/2397 [==============================] - 153s 64ms/step - loss: 0.0015\n",
      "Epoch 67/100\n",
      "2397/2397 [==============================] - 163s 68ms/step - loss: 0.0014\n",
      "Epoch 68/100\n",
      "2397/2397 [==============================] - 152s 64ms/step - loss: 0.0013\n",
      "Epoch 69/100\n",
      "2397/2397 [==============================] - 158s 66ms/step - loss: 0.0015\n",
      "Epoch 70/100\n",
      "2397/2397 [==============================] - 152s 63ms/step - loss: 0.0014\n",
      "Epoch 71/100\n",
      "2397/2397 [==============================] - 156s 65ms/step - loss: 0.0013\n",
      "Epoch 72/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0013\n",
      "Epoch 73/100\n",
      "2397/2397 [==============================] - 153s 64ms/step - loss: 0.0013\n",
      "Epoch 74/100\n",
      "2397/2397 [==============================] - 159s 67ms/step - loss: 0.0013\n",
      "Epoch 75/100\n",
      "2397/2397 [==============================] - 156s 65ms/step - loss: 0.0013\n",
      "Epoch 76/100\n",
      "2397/2397 [==============================] - 162s 68ms/step - loss: 0.0013\n",
      "Epoch 77/100\n",
      "2397/2397 [==============================] - 160s 67ms/step - loss: 0.0013\n",
      "Epoch 78/100\n",
      "2397/2397 [==============================] - 155s 65ms/step - loss: 0.0013\n",
      "Epoch 79/100\n",
      "2397/2397 [==============================] - 153s 64ms/step - loss: 0.0013\n",
      "Epoch 80/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0012\n",
      "Epoch 81/100\n",
      "2397/2397 [==============================] - 154s 64ms/step - loss: 0.0012\n",
      "Epoch 82/100\n",
      "2397/2397 [==============================] - 166s 69ms/step - loss: 0.0012\n",
      "Epoch 83/100\n",
      "2397/2397 [==============================] - 162s 67ms/step - loss: 0.0012\n",
      "Epoch 84/100\n",
      "2397/2397 [==============================] - 162s 68ms/step - loss: 0.0012\n",
      "Epoch 85/100\n",
      "2397/2397 [==============================] - 158s 66ms/step - loss: 0.0013\n",
      "Epoch 86/100\n",
      "2397/2397 [==============================] - 151s 63ms/step - loss: 0.0012\n",
      "Epoch 87/100\n",
      "2397/2397 [==============================] - 151s 63ms/step - loss: 0.0012\n",
      "Epoch 88/100\n",
      "2397/2397 [==============================] - 165s 69ms/step - loss: 0.0012\n",
      "Epoch 89/100\n",
      "2397/2397 [==============================] - 160s 67ms/step - loss: 0.0013\n",
      "Epoch 90/100\n",
      "2397/2397 [==============================] - 158s 66ms/step - loss: 0.0012\n",
      "Epoch 91/100\n",
      "2397/2397 [==============================] - 157s 66ms/step - loss: 0.0012\n",
      "Epoch 92/100\n",
      "2397/2397 [==============================] - 157s 66ms/step - loss: 0.0011\n",
      "Epoch 93/100\n",
      "2397/2397 [==============================] - 158s 66ms/step - loss: 0.0012\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2397/2397 [==============================] - 177s 74ms/step - loss: 0.0012\n",
      "Epoch 95/100\n",
      "2397/2397 [==============================] - 197s 82ms/step - loss: 0.0012\n",
      "Epoch 96/100\n",
      "2397/2397 [==============================] - 195s 81ms/step - loss: 0.0011\n",
      "Epoch 97/100\n",
      " 192/2397 [=>............................] - ETA: 2:48 - loss: 0.0011   "
     ]
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Google Stock Price Test set for January 2017 using pandas and make it a numpy array.\n",
    "\n",
    "There are 20 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the real stock price for February 1st 2012 - \n",
    "# January 31st 2017\n",
    "\n",
    "dataset_test = pd.read_csv('dataset_test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in January 2017, we need the values in the last 60 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2017, and we need to go back 60 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 120:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each price in Jan. 2017 we need the **immediate 60 values** before it. \n",
    "2. We have 21 prices in January;\n",
    "3. We need a numpy 3D array of 60 prices (columns) times 21 days (rows) times 1 dependent variable \n",
    "4. We don’t need y_test. That is what we are trying to compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 60 from inputs are from training set; start \n",
    "# from 60 and get the extra 20, i.e. up to 80\n",
    "for i in range(120, 111): \n",
    "    X_test.append(inputs[i-120:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Exxon Mobil Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Exxon Mobil Stock Price')\n",
    "\n",
    "plt.title('Exxon Mobil Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Exxon Mobil Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of January 2017. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Google Stock Price values of January 2017 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:21,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 5 years of the  Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 time steps (3 months). You could try to increase the number of time steps, by choosing for example 120 time steps (6 months).\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
